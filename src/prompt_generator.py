from src.utils.prompts import DEFAULT_EXPLANATION_PROMPT, DEFAULT_LABEL_PROMPT, DEFAULT_FILE_PROMPT
import polars as pl
from src.llm_prompter import LLMPrompter
import os

class PromptGenerator:
    def __init__(self):
        # Default prompt templates
        self.explanation_prompt = DEFAULT_EXPLANATION_PROMPT
        self.label_prompt = DEFAULT_LABEL_PROMPT
        # Template for the combined file anomaly identification & explanation task
        self.file_prompt = DEFAULT_FILE_PROMPT

        # Use LLMPrompter to generate label if not already present
        self.prompter = LLMPrompter()

    def generateLabelPrompts(self, threshold, df, prompt_template=None):
        """
        Generates prompts for getting labels and adds them as a new column in the DataFrame.
        Modified to better handle context from ContextSelection.

        Parameters:
        - threshold (float): The anomaly score threshold for identifying anomalous lines.
        - df (pl.DataFrame): Polars DataFrame containing log data with columns including 'anomaly_score',
                            'LineId', 'context_ids_ref', and optionally other headers.
        - prompt_template (str, optional): Custom prompt template to use. Defaults to self.label_prompt.

        Returns:
        - df (pl.DataFrame): Updated DataFrame with a new 'anomaly_label' column for anomalous lines containing prompts.
        """
        headers = df.columns

        # Initialize the 'anomaly_label' column with null
        df = df.with_columns(pl.lit(None).alias('anomaly_label'))

        # Filter rows where anomaly score meets or exceeds the threshold
        anomaly_rows = df.filter(pl.col('anomaly_score') >= threshold)

        # Use default prompt if none provided
        if prompt_template is None:
            prompt_template = self.label_prompt

        # Process each anomalous row
        for row in anomaly_rows.iter_rows(named=True):
            idx = row['LineId']  # Using LineId as identifier
            anomaly_lineid = str(row['LineId'])  # Convert to string for comparison

            # Format the log line as semicolon-separated key-value pairs
            log_str = "; ".join([f"{header}: {row[header]}" for header in headers 
                                if header not in ['anomaly_label', 'explanation_prompt', 'explanation_result', 'anomaly_result']])

            # Get context lines using context_ids_ref column - ensure string comparison
            try:
                # First try to compare as strings (if context_ids_ref is string)
                context_df = df.filter(pl.col('context_ids_ref').cast(pl.Utf8) == anomaly_lineid)
                if context_df.is_empty():
                    # Try numeric comparison if string comparison yields no results
                    context_df = df.filter(pl.col('context_ids_ref').cast(pl.Int32) == int(anomaly_lineid))
            except:
                # Default to empty DataFrame if all else fails
                context_df = df.filter(pl.col('LineId') < 0)  # Empty filter
            
            context_str = "No context lines available." if context_df.is_empty() else "\n".join(
                ["; ".join([f"{header}: {context_row[header]}" for header in headers 
                          if header not in ['anomaly_label', 'explanation_prompt', 'explanation_result', 'anomaly_result']]) 
                 for context_row in context_df.iter_rows(named=True)]
            )
            
            # Populate the prompt with the log line and context
            prompt = prompt_template.format(
                log_str=log_str,
                context_str=context_str
            )

            # Update the DataFrame with the prompt - ensure consistent types
            df = df.with_columns(
                pl.when(pl.col('LineId') == idx)
                .then(pl.lit(prompt))
                .otherwise(pl.col('anomaly_label'))
                .alias('anomaly_label')
            )

        return df

    def generateExplanationPrompts(self, threshold, df, prompt_template=None):
        """
        Generates prompts for getting explanations from an LLM and adds them as a new column in the DataFrame.
        Prompts are generated only for lines with an anomaly score greater than or equal to the threshold.
        Modified to better handle context from ContextSelection.

        Parameters:
        - threshold (float): The anomaly score threshold for identifying anomalous lines.
        - df (pl.DataFrame): Polars DataFrame containing log data with columns including 'anomaly_score',
                            'LineId', 'context_ids_ref', 'anomaly_result' (label generated by LLMPrompter),
                            and optionally 'label' with actual labels (external).
        - prompt_template (str, optional): Custom prompt template to use. Defaults to self.explanation_prompt.

        Returns:
        - df (pl.DataFrame): Updated DataFrame with a new 'explanation_prompt' column for anomalous lines.
        """
        headers = df.columns

        # Initialize the 'explanation_prompt' column with null
        df = df.with_columns(pl.lit(None).alias('explanation_prompt'))

        # Filter rows where anomaly score meets or exceeds the threshold
        anomaly_rows = df.filter(pl.col('anomaly_score') >= threshold)

        # Ensure labels are generated if not already present
        if 'label' not in df.columns and 'anomaly_result' not in df.columns:
            # Generate labels first
            df = self.generateLabelPrompts(threshold, df)
            df = self.prompter.getLabelResponses(df)

        # Use default prompt if none provided
        if prompt_template is None:
            prompt_template = self.explanation_prompt

        # Process each anomalous row
        for row in anomaly_rows.iter_rows(named=True):
            idx = row['LineId']
            anomaly_lineid = str(row['LineId'])  # Convert to string for comparison

            # Format the anomalous log as semicolon-separated key-value pairs
            anomalous_log_str = "; ".join([f"{header}: {row[header]}" for header in headers 
                                         if header not in ['anomaly_label', 'explanation_prompt', 'explanation_result', 'anomaly_result']])

            # Get context lines using context_ids_ref column - ensure string comparison
            try:
                # Try string comparison first
                context_df = df.filter(pl.col('context_ids_ref').cast(pl.Utf8) == anomaly_lineid)
                if context_df.is_empty():
                    # Try numeric comparison if string comparison yields no results
                    context_df = df.filter(pl.col('context_ids_ref').cast(pl.Int32) == int(anomaly_lineid))
            except:
                # Default to empty DataFrame if all else fails
                context_df = df.filter(pl.col('LineId') < 0)  # Empty filter
            
            context_str = "No context lines available." if context_df.is_empty() else "\n".join(
                ["; ".join([f"{header}: {context_row[header]}" for header in headers 
                          if header not in ['anomaly_label', 'explanation_prompt', 'explanation_result', 'anomaly_result']]) 
                 for context_row in context_df.iter_rows(named=True)]
            )
            
            # Get the anomaly label
            if 'label' in df.columns and row['label'] is not None:
                label_str = f"Anomaly Label: {row['label']}"
            elif 'anomaly_result' in df.columns and row['anomaly_result'] is not None:
                label_str = f"Anomaly Label: {row['anomaly_result']}"
            else:
                label_str = "Anomaly Label: unknown"

            # Format the prompt
            prompt = prompt_template.format(
                anomalous_log_str=anomalous_log_str,
                label_str=label_str,
                context_str=context_str
            )

            # Update the DataFrame with the prompt
            df = df.with_columns(
                pl.when(pl.col('LineId') == idx)
                .then(pl.lit(prompt))
                .otherwise(pl.col('explanation_prompt'))
                .alias('explanation_prompt')
            )

        return df
    
    # Helper: Formatter for the file combined prompt
    def format_log_lines(self, df_context: pl.DataFrame) -> str | None:
        """Formats log lines for file anomaly identification & explanation."""
        # Required columns for display: message, score
        if "pred_ano_proba" not in df_context.columns:
                print(f"Error: Formatting requires 'pred_ano_proba'. Found: {df_context.columns}")
                return None
        msg_col = "m_message"
        if msg_col not in df_context.columns:
                print(f"Error: Formatting requires message column ('{msg_col}'). Found: {df_context.columns}")
                return None

        formatted_lines = []
        for row in df_context.select([msg_col, "pred_ano_proba"]).iter_rows(named=True):
            score = row.get("pred_ano_proba", "N/A")
            msg = str(row.get(msg_col, "")).strip()
            msg_snippet = (msg[:150] + '...') if len(msg) > 150 else msg # Concise snippet

            try:
                    score_fmt = f"{float(score):.4f}"
            except (ValueError, TypeError, SystemError):
                    score_fmt = str(score)

            formatted_lines.append(f"(Score: {score_fmt}) Msg: {msg_snippet}")

        return "\n".join(formatted_lines)

    # File Prompt Generator
    def generate_file_explanation_prompt(self, filename: str, df_context: pl.DataFrame, prompt_template=None) -> str | None:
        """
        Generates a single prompt for file anomaly identification & explanation.

        Parameters:
        - filename (str): The full path or basename of the log file.
        - df_context (pl.DataFrame): DataFrame context containing
                                        message column and 'pred_ano_proba'.
        - prompt_template (str, optional): Custom prompt template. Defaults to
                                            self.file_prompt.

        Returns:
        - str | None: The generated prompt string, or None on error.
        """
        if df_context is None:
            print("Error: Cannot generate file explanation prompt with empty context.")
            return None

        # Filename Parsing, e.g. "TOKEN_access_token_auth_header_error_401.log.parquet"
        base_filename = os.path.basename(filename)
        error_type = "Unknown"
        try:
            parts = base_filename.split('_')
            if len(parts) > 3 and parts[-1].endswith(('.log', '.parquet')):
                meaningful_parts = parts[1:-2] # Exclude 'TOKEN' and '.log.parquet'
                error_type = ' '.join(meaningful_parts).strip()
                if not error_type: error_type = "No anomaly in filename"
            else:
                    name_part = base_filename.split('.')[0]; error_type = name_part.replace('_', ' ').replace('TOKEN', '').strip()
        except Exception as e:
            print(f"Warning: Error parsing filename '{base_filename}': {e}")
            error_type = "error suspected based on filename"

        # Format Log Lines
        formatted_logs_str = self.format_log_lines(df_context)
        if formatted_logs_str is None:
            print("Error: Failed to format log lines for the prompt.")
            return None

        # Generates a single prompt for file anomaly identification & explanation
        template_to_use = prompt_template if prompt_template else self.file_prompt
        try:
            final_prompt = template_to_use.format(
                filename=base_filename,
                error_type=error_type,
                formatted_log_lines=formatted_logs_str 
            )
            return final_prompt
        except KeyError as e:
                print(f"Error formatting combined prompt template. Missing key: {e}")
                return None
        except Exception as e:
            print(f"An unexpected error occurred during combined prompt formatting: {e}")
            return None

    def getContextLines(self, df):
        # This is now handled by the ContextSelection class
        return df
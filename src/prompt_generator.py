from src.utils.prompts import DEFAULT_EXPLANATION_PROMPT, DEFAULT_LABEL_PROMPT
import polars as pl
from src.llm_prompter import LLMPrompter

class PromptGenerator:
    def __init__(self):
        # Default prompt templates
        self.explanation_prompt = DEFAULT_EXPLANATION_PROMPT
        self.label_prompt = DEFAULT_LABEL_PROMPT

        # Use LLMPrompter to generate label if not already present
        self.prompter = LLMPrompter()

    def generateLabelPrompts(self, threshold, df, prompt_template=None):
        """
        Generates prompts for getting labels and adds them as a new column in the DataFrame.
        Modified to better handle context from ContextSelection.

        Parameters:
        - threshold (float): The anomaly score threshold for identifying anomalous lines.
        - df (pl.DataFrame): Polars DataFrame containing log data with columns including 'anomaly_score',
                            'LineId', 'lexical_context', and optionally other headers.
        - prompt_template (str, optional): Custom prompt template to use. Defaults to self.label_prompt.

        Returns:
        - df (pl.DataFrame): Updated DataFrame with a new 'anomaly_label' column for anomalous lines containing prompts.
        """
        headers = df.columns

        # Initialize the 'anomaly_label' column with null
        df = df.with_columns(pl.lit(None).alias('anomaly_label'))

        # Filter rows where anomaly score meets or exceeds the threshold
        anomaly_rows = df.filter(pl.col('anomaly_score') >= threshold)

        # Use default prompt if none provided
        if prompt_template is None:
            prompt_template = self.label_prompt

        # Process each anomalous row
        for row in anomaly_rows.iter_rows(named=True):
            idx = row['LineId']  # Using LineId as identifier
            anomaly_lineid = str(row['LineId'])  # Convert to string for comparison

            # Format the log line as semicolon-separated key-value pairs
            log_str = "; ".join([f"{header}: {row[header]}" for header in headers 
                                if header not in ['anomaly_label', 'explanation_prompt', 'explanation_result', 'anomaly_result']])

            # Get context lines using lexical_context column - ensure string comparison
            try:
                # First try to compare as strings (if lexical_context is string)
                context_df = df.filter(pl.col('lexical_context').cast(pl.Utf8) == anomaly_lineid)
                if context_df.is_empty():
                    # Try numeric comparison if string comparison yields no results
                    context_df = df.filter(pl.col('lexical_context').cast(pl.Int32) == int(anomaly_lineid))
            except:
                # Default to empty DataFrame if all else fails
                context_df = df.filter(pl.col('LineId') < 0)  # Empty filter
            
            context_str = "No context lines available." if context_df.is_empty() else "\n".join(
                ["; ".join([f"{header}: {context_row[header]}" for header in headers 
                          if header not in ['anomaly_label', 'explanation_prompt', 'explanation_result', 'anomaly_result']]) 
                 for context_row in context_df.iter_rows(named=True)]
            )
            
            # Populate the prompt with the log line and context
            prompt = prompt_template.format(
                log_str=log_str,
                context_str=context_str
            )

            # Update the DataFrame with the prompt - ensure consistent types
            df = df.with_columns(
                pl.when(pl.col('LineId') == idx)
                .then(pl.lit(prompt))
                .otherwise(pl.col('anomaly_label'))
                .alias('anomaly_label')
            )

        return df

    def generateExplanationPrompts(self, threshold, df, prompt_template=None):
        """
        Generates prompts for getting explanations from an LLM and adds them as a new column in the DataFrame.
        Prompts are generated only for lines with an anomaly score greater than or equal to the threshold.
        Modified to better handle context from ContextSelection.

        Parameters:
        - threshold (float): The anomaly score threshold for identifying anomalous lines.
        - df (pl.DataFrame): Polars DataFrame containing log data with columns including 'anomaly_score',
                            'LineId', 'lexical_context', 'anomaly_result' (label generated by LLMPrompter),
                            and optionally 'label' with actual labels (external).
        - prompt_template (str, optional): Custom prompt template to use. Defaults to self.explanation_prompt.

        Returns:
        - df (pl.DataFrame): Updated DataFrame with a new 'explanation_prompt' column for anomalous lines.
        """
        headers = df.columns

        # Initialize the 'explanation_prompt' column with null
        df = df.with_columns(pl.lit(None).alias('explanation_prompt'))

        # Filter rows where anomaly score meets or exceeds the threshold
        anomaly_rows = df.filter(pl.col('anomaly_score') >= threshold)

        # Ensure labels are generated if not already present
        if 'label' not in df.columns and 'anomaly_result' not in df.columns:
            # Generate labels first
            df = self.generateLabelPrompts(threshold, df)
            df = self.prompter.getLabelResponses(df)

        # Use default prompt if none provided
        if prompt_template is None:
            prompt_template = self.explanation_prompt

        # Process each anomalous row
        for row in anomaly_rows.iter_rows(named=True):
            idx = row['LineId']
            anomaly_lineid = str(row['LineId'])  # Convert to string for comparison

            # Format the anomalous log as semicolon-separated key-value pairs
            anomalous_log_str = "; ".join([f"{header}: {row[header]}" for header in headers 
                                         if header not in ['anomaly_label', 'explanation_prompt', 'explanation_result', 'anomaly_result']])

            # Get context lines using lexical_context column - ensure string comparison
            try:
                # Try string comparison first
                context_df = df.filter(pl.col('lexical_context').cast(pl.Utf8) == anomaly_lineid)
                if context_df.is_empty():
                    # Try numeric comparison if string comparison yields no results
                    context_df = df.filter(pl.col('lexical_context').cast(pl.Int32) == int(anomaly_lineid))
            except:
                # Default to empty DataFrame if all else fails
                context_df = df.filter(pl.col('LineId') < 0)  # Empty filter
            
            context_str = "No context lines available." if context_df.is_empty() else "\n".join(
                ["; ".join([f"{header}: {context_row[header]}" for header in headers 
                          if header not in ['anomaly_label', 'explanation_prompt', 'explanation_result', 'anomaly_result']]) 
                 for context_row in context_df.iter_rows(named=True)]
            )
            
            # Get the anomaly label
            if 'label' in df.columns and row['label'] is not None:
                label_str = f"Anomaly Label: {row['label']}"
            elif 'anomaly_result' in df.columns and row['anomaly_result'] is not None:
                label_str = f"Anomaly Label: {row['anomaly_result']}"
            else:
                label_str = "Anomaly Label: unknown"

            # Format the prompt
            prompt = prompt_template.format(
                anomalous_log_str=anomalous_log_str,
                label_str=label_str,
                context_str=context_str
            )

            # Update the DataFrame with the prompt
            df = df.with_columns(
                pl.when(pl.col('LineId') == idx)
                .then(pl.lit(prompt))
                .otherwise(pl.col('explanation_prompt'))
                .alias('explanation_prompt')
            )

        return df

    def getContextLines(self, df):
        # This is now handled by the ContextSelection class
        return df
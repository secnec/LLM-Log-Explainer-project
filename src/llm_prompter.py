import os
import pandas as pd
import polars as pl
from dotenv import load_dotenv
from utils.anomaly import AnomalyLabeler
import dspy
class LLMPrompter:
    def __init__(self):
        self.api_key = None
        self.lm = None
        try:
            load_dotenv()
            self.api_key = os.getenv("OPENROUTER_API_KEY")
        except:
            raise Exception("Failed to init OpenRouter API")

    def updateLM(self, new_lm):
        self.lm = dspy.LM(new_lm, self.api_key)
        dspy.configure(lm=self.lm)

    def loadLM(self, path="./utils/classifier_optimize.json"):
        loaded_dspy_program = dspy.ChainOfThought(AnomalyLabeler)
        loaded_dspy_program.load(path)

    def getExplanationResponses(self, df):
        """
        Gets the responses from the LLM for each prompt in the 'explanation_prompt' column,
        and writes them into a new 'explanation_result' column.

        Parameters:
        - df (pl.DataFrame): Polars DataFrame containing log data with an 'explanation_prompt' column.

        Returns:
        - df (pl.DataFrame): Updated DataFrame with a new 'explanation_result' column containing the LLM responses.
        """
        # Ensure LLM is configured
        if not hasattr(dspy.settings, 'lm'):
            self.updateLM("openrouter/openai/gpt-4o-mini")
        
        # Filter rows with non-null explanation prompts
        rows_with_prompts = df.filter(pl.col('explanation_prompt').is_not_null())
        
        # List to store generated explanations
        explanations = []
        for row in rows_with_prompts.iter_rows(named=True):
            prompt = row['explanation_prompt']
            try:
                # Call the LLM with the prompt, setting max_tokens to limit response length
                response = self.lm(prompt, max_tokens=300)
                # Extract the first response if it exists, otherwise use a default message
                explanation = response[0] if isinstance(response, list) and response else "No explanation generated."
            except Exception as e:
                # Handle any errors by storing an error message
                explanation = f"Error: {str(e)}"
            explanations.append(explanation)
        
        # Create a temporary DataFrame with LineId and explanations
        explanation_df = pl.DataFrame({
            'LineId': rows_with_prompts['LineId'],
            'explanation_result': explanations
        })
        
        # Join the temporary DataFrame with the original DataFrame on 'LineId'
        df = df.join(explanation_df, on='LineId', how='left')
        
        return df

    def getLabelResponses(self, df, client_dspy=True):
        """
        Gets the labels from the LLM for each prompt in the anomaly_label column,
        and writes them into a new anomaly_result column.
        
        Parameters:
        - df (pl.DataFrame): Polars DataFrame containing an 'anomaly_label' column with prompts
                            (generated by PromptGenerator.generateLabelPrompts)
        - client_dspy (bool): Whether to use the DSPy model (True) or raw API calls (False)
        
        Returns:
        - df (pl.DataFrame): Updated DataFrame with a new 'anomaly_result' column containing the LLM responses
        """
        # Initialize the 'anomaly_result' column with null values
        df = df.with_columns(pl.lit(None).alias('anomaly_result'))
        
        # Valid label categories from AnomalyLabeler signature
        valid_labels = ['application', 'authentication', 'io', 'memory', 'network', 'other']
        
        # Load the DSPy model or configure a basic LM
        if client_dspy:
            self.loadLM()
            anomaly_labeler = dspy.ChainOfThought(AnomalyLabeler)
        else:
            # If not using DSPy, configure a basic LM
            model = "openrouter/openai/gpt-4o-mini"
            lm = dspy.LM(model, api_key=self.api_key)
            dspy.configure(lm=lm)
        
        # Filter rows that have a prompt in the anomaly_label column
        rows_with_prompts = df.filter(pl.col('anomaly_label').is_not_null())
        
        # Process each row with a prompt
        for row in rows_with_prompts.iter_rows(named=True):
            idx = row['LineId']
            prompt = row['anomaly_label']  # This already contains the properly formatted prompt
            
            try:
                if client_dspy:
                    # Use the DSPy model for prediction
                    prediction = anomaly_labeler(text=prompt) ## Fix this later
                    label = prediction.label
                else:
                    # Use the pre-generated prompt directly with the LM
                    response = lm(prompt)
                    
                    # Extract the label from the response
                    response_text = response.strip().lower()
                    # Find which valid label is mentioned in the response
                    label = next((l for l in valid_labels if l in response_text), 'other')
                
                # Update the DataFrame with the label
                df = df.with_columns(
                    pl.when(pl.col('LineId') == idx)
                    .then(pl.lit(label))
                    .otherwise(pl.col('anomaly_result'))
                    .alias('anomaly_result')
                )
            except Exception as e:
                print(f"Error processing LineId {idx}: {str(e)}")
                # Default to 'other' in case of errors
                df = df.with_columns(
                    pl.when(pl.col('LineId') == idx)
                    .then(pl.lit('other'))
                    .otherwise(pl.col('anomaly_result'))
                    .alias('anomaly_result')
                )
        
        return df

    def get_file_explanation_response(self, file_explanation_prompt: str) -> str | None:
        """
        Gets a single explanation response from the LLM for a file-level prompt.
        """
        if not hasattr(dspy.settings, 'lm'):
            self.updateLM("openrouter/openai/gpt-4o-mini")
        if not file_explanation_prompt: return "Error: Received an empty prompt."
        try:
            print("Sending combined file prompt to LLM...")
            # Adjust max_tokens if needed for combined identify/explain
            response = self.lm(file_explanation_prompt, max_tokens=800) # Potentially longer response
            # Handle response list/string
            if isinstance(response, list) and response: return response[0].strip()
            if isinstance(response, str): return response.strip()
            return f"LLM returned no valid response. Raw: {response}"
        except Exception as e:
            print(f"Error getting file explanation from LLM: {e}")
            return f"Error during LLM communication: {str(e)}"